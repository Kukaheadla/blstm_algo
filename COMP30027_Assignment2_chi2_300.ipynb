{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd0ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc",
   "display_name": "Python 3.9.2 64-bit ('python@3.9')"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "from sklearn import linear_model, naive_bayes\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold, train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "import tensorflow as tf \n",
    "import tensorflow.keras.utils as utils\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, LSTM, Bidirectional\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "SEED_NO=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Training Data\n",
    "X_train_name = scipy.sparse.load_npz(r'./COMP30027_2021_Project2_datasets/recipe_text_features_countvec/train_ingr_vec.npz')\n",
    "X_train_steps = scipy.sparse.load_npz(r'./COMP30027_2021_Project2_datasets/recipe_text_features_countvec/train_ingr_vec.npz')\n",
    "X_train_ingr = scipy.sparse.load_npz(r'./COMP30027_2021_Project2_datasets/recipe_text_features_countvec/train_steps_vec.npz')\n",
    "\n",
    "# Load the Test Data\n",
    "X_test_name = scipy.sparse.load_npz(r'./COMP30027_2021_Project2_datasets/recipe_text_features_countvec/test_ingr_vec.npz')\n",
    "X_test_steps = scipy.sparse.load_npz(r'./COMP30027_2021_Project2_datasets/recipe_text_features_countvec/test_ingr_vec.npz')\n",
    "X_test_ingr = scipy.sparse.load_npz(r'./COMP30027_2021_Project2_datasets/recipe_text_features_countvec/test_steps_vec.npz')\n",
    "\n",
    "#Grab Original Label\n",
    "X_train_original = pd.read_csv(r\"./COMP30027_2021_Project2_datasets/recipe_train.csv\", index_col = False, delimiter = ',', header=0)\n",
    "y_train = X_train_original.duration_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kbest_name =  SelectKBest(chi2, k=300).fit(X_train_name, y_train)\n",
    "Kbest_steps = SelectKBest(chi2, k=300).fit(X_train_steps, y_train)\n",
    "Kbest_ingr = SelectKBest(chi2, k=300).fit(X_train_ingr, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_name, X_train_steps, X_train_ingr = Kbest_name.transform(X_train_name), Kbest_steps.transform(X_train_steps), Kbest_ingr.transform(X_train_ingr)\n",
    "X_test_name, X_test_steps, X_test_ingr = Kbest_name.transform(X_test_name), Kbest_steps.transform(X_test_steps), Kbest_ingr.transform(X_test_ingr)\n",
    "\n",
    "X_train = np.concatenate((X_train_name.toarray(), X_train_steps.toarray(), X_train_ingr.toarray()), axis=1)\n",
    "X_test = np.concatenate((X_test_name.toarray(), X_test_steps.toarray(), X_test_ingr.toarray()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(10000, 900)\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.DataFrame(X_train)\n",
    "\n",
    "# Normalize the data to optimize for high learning rate. \n",
    "sc = StandardScaler()\n",
    "X_train, X_test = sc.fit_transform(X_train), sc.fit_transform(X_test)\n",
    "X_train, X_test = pd.DataFrame(X_train), pd.DataFrame(X_test)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross Validation Accuracy for Naive Bayes:  0.658425\n"
     ]
    }
   ],
   "source": [
    "NB_clf = naive_bayes.GaussianNB()\n",
    "NB_accuracy = cross_val_score(NB_clf, X_train, y_train, cv=5).mean()\n",
    "print(\"Cross Validation Accuracy for Naive Bayes: \", NB_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross Validation Accuracy for Logistic Regression:  0.7986\n"
     ]
    }
   ],
   "source": [
    "LR_clf = linear_model.LogisticRegression(random_state=SEED_NO,\n",
    "                                         C=0.9, \n",
    "                                         max_iter = 50000,\n",
    "                                   multi_class='multinomial')\n",
    "lr_fit = LR_clf.fit(X_train, y_train)\n",
    "LR_accuracy = cross_val_score(LR_clf, X_train, y_train, cv=5).mean()\n",
    "print(\"Cross Validation Accuracy for Logistic Regression: \", LR_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Transformed X_train Dimensions:           (40000, 1, 900)\nOne-hot Encoding Dimensions for y_train:  (40000, 3)\n"
     ]
    }
   ],
   "source": [
    "# #Stacking all Vectorized feature and reshape them into  3-D Array\n",
    "X_train_3dim = np.reshape(np.array(X_train), newshape=(X_train.shape[0], 1, X_train.shape[1]))\n",
    "print(\"Transformed X_train Dimensions:          \", X_train_3dim.shape)\n",
    "\n",
    "# One-hot Encoding for Y_train\n",
    "y_train_le = LabelEncoder().fit_transform(y_train)\n",
    "y_train_onehot = utils.to_categorical(y_train_le)\n",
    "print(\"One-hot Encoding Dimensions for y_train: \", y_train_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bidirectional_LSTM_clf(X, y, epochs_size, batch_size):\n",
    "    model = Sequential()\n",
    "\n",
    "    #Bidirectional LSTM\n",
    "    model.add(Bidirectional(LSTM(X.shape[2], return_sequences=True, dropout=0.45, input_shape=(1, X.shape[2]))))\n",
    "    model.add(Bidirectional(LSTM(X.shape[2],return_sequences=False, dropout=0.45, input_shape=(1, X.shape[2]))))\n",
    "\n",
    "    #Adding dense layer to implement activation layer \n",
    "    model.add(Dense(X.shape[2], activation='tanh'))\n",
    "    model.add(Dense(X.shape[2]*2, activation='relu'))\n",
    "    model.add(Dense(X.shape[2], activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax')) #Softmax output for 3 corresponding categorical variables\n",
    "\n",
    "    #Implement Loss function w.r.t probabiltiy over possible classes\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model_history = model.fit(X, y, \n",
    "                            epochs=epochs_size, \n",
    "                            batch_size=batch_size, \n",
    "                            validation_split=0.2, #Only 80% of input data will be trained for diagnostics purposes\n",
    "                            verbose=1)\n",
    "    return model, model_history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi_lstm_model.summary()\n",
    "# print(\"Bidirectional LSTM Model Training Acccuracy:          {:.2f}\".format(bi_lstm_model_history.history['accuracy'][-1]))\n",
    "# print(\"Bidirectional LSTM Model Cross-Validation Acccuracy:  {:.2f}\".format(bi_lstm_model_history.history['val_accuracy'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Transformed X_train Dimensions:           (28000, 1, 900)\n",
      "One-hot Encoding Dimensions for y_train:  (28000, 3)\n",
      "Epoch 1/10\n",
      "175/175 [==============================] - 84s 427ms/step - loss: 0.6099 - accuracy: 0.7378 - val_loss: 0.5600 - val_accuracy: 0.7686\n",
      "Epoch 2/10\n",
      "175/175 [==============================] - 78s 445ms/step - loss: 0.5137 - accuracy: 0.7796 - val_loss: 0.5039 - val_accuracy: 0.7887\n",
      "Epoch 3/10\n",
      "175/175 [==============================] - 55s 315ms/step - loss: 0.4807 - accuracy: 0.7910 - val_loss: 0.4879 - val_accuracy: 0.7957\n",
      "Epoch 4/10\n",
      "175/175 [==============================] - 64s 365ms/step - loss: 0.4557 - accuracy: 0.8023 - val_loss: 0.4925 - val_accuracy: 0.7864\n",
      "Epoch 5/10\n",
      "175/175 [==============================] - 76s 433ms/step - loss: 0.4385 - accuracy: 0.8117 - val_loss: 0.4981 - val_accuracy: 0.7884\n",
      "Epoch 6/10\n",
      "175/175 [==============================] - 50s 286ms/step - loss: 0.4158 - accuracy: 0.8195 - val_loss: 0.4978 - val_accuracy: 0.7900\n",
      "Epoch 7/10\n",
      "175/175 [==============================] - 45s 257ms/step - loss: 0.3939 - accuracy: 0.8323 - val_loss: 0.5027 - val_accuracy: 0.7914\n",
      "Epoch 8/10\n",
      "175/175 [==============================] - 44s 250ms/step - loss: 0.3735 - accuracy: 0.8396 - val_loss: 0.5167 - val_accuracy: 0.7898\n",
      "Epoch 9/10\n",
      "175/175 [==============================] - 43s 249ms/step - loss: 0.3520 - accuracy: 0.8501 - val_loss: 0.5610 - val_accuracy: 0.7912\n",
      "Epoch 10/10\n",
      "175/175 [==============================] - 43s 247ms/step - loss: 0.3282 - accuracy: 0.8622 - val_loss: 0.5328 - val_accuracy: 0.7830\n"
     ]
    }
   ],
   "source": [
    "target_names =['1.0', '2.0', '3.0']\n",
    "\n",
    "#Split the data to test for LSTM accuracy \n",
    "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(X_train, y_train, test_size=0.3, random_state=SEED_NO)\n",
    "\n",
    "# #Stacking all Vectorized feature and reshape them into  3-D Array\n",
    "X_train_3dim = np.reshape(np.array(X_train_lstm), newshape=(X_train_lstm.shape[0], 1, X_train_lstm.shape[1]))\n",
    "X_test_3dim = np.reshape(np.array(X_test_lstm), newshape=(X_test_lstm.shape[0], 1, X_test_lstm.shape[1]))\n",
    "print(\"Transformed X_train Dimensions:          \", X_train_3dim.shape)\n",
    "\n",
    "# One-hot Encoding for Y_train\n",
    "y_train_le = LabelEncoder().fit_transform(y_train_lstm)\n",
    "y_train_onehot = utils.to_categorical(y_train_le)\n",
    "print(\"One-hot Encoding Dimensions for y_train: \", y_train_onehot.shape)\n",
    "\n",
    "bi_lstm_model, bi_lstm_model_history = Bidirectional_LSTM_clf(X_train_3dim, y_train_onehot, 10, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------- Bidirectional LSTM Classifier Report -------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0     0.7775    0.7594    0.7684      5283\n",
      "         2.0     0.7857    0.8126    0.7989      6110\n",
      "         3.0     0.7140    0.6129    0.6596       607\n",
      "\n",
      "    accuracy                         0.7791     12000\n",
      "   macro avg     0.7591    0.7283    0.7423     12000\n",
      "weighted avg     0.7785    0.7791    0.7784     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"------- Bidirectional LSTM Classifier Report -------\")\n",
    "y_pred_BLSTM = np.array(list(map(lambda x: float(x+1), # Increment output to match category as np.argmax return position of one-hot array\n",
    "                        np.argmax(bi_lstm_model.predict(X_test_3dim), axis=-1))))\n",
    "print(classification_report(y_test_lstm.astype(str), y_pred_BLSTM.astype(str), target_names=target_names, digits=4))"
   ]
  },
  {
   "source": [
    "## Export prediction labels to CSV"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_pred_to_csv(y_pred, fname):\n",
    "    pd.DataFrame(zip(np.arange(1, len(y_pred)+1), y_pred), columns=[\"id\", \"duration_label\"]).to_csv(\"{}\".format(fname), header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using Naive Bayes\n",
    "NB_clf.fit(X_train, y_train)\n",
    "y_pred_NB = NB_clf.predict(X_test)\n",
    "\n",
    "# Predict using Logistic Regression\n",
    "LR_clf.fit(X_train, y_train)\n",
    "y_pred_LR = LR_clf.predict(X_test)\n",
    "\n",
    "#Predict using BLSTM \n",
    "y_pred_BLSTM = np.array(list(map(lambda x: float(x+1), # Increment output to match category as np.argmax return position of one-hot array\n",
    "                        np.argmax(bi_lstm_model.predict(X_test_3dim), axis=-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_pred_to_csv(y_pred_BLSTM, \"BLSTM_y_pred_chi300.csv\")\n",
    "export_pred_to_csv(y_pred_NB, \"NB_y_pred_chi300.csv\")\n",
    "export_pred_to_csv(y_pred_LR, \"LR_y_pred_chi300.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}